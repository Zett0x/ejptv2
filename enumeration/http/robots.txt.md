# robots.txt

## Introducci√≥n
- Archivo de texto ubicado en la ra√≠z del sitio:  
  ```
  https://www.example.com/robots.txt
  ```
- Basado en el **Robots Exclusion Standard**.  
- Define reglas (directivas) que indican a los bots **qu√© pueden y qu√© no pueden rastrear**.  
- Funciona como una ‚Äúgu√≠a de etiqueta‚Äù para crawlers.  

---

## Estructura de robots.txt
Un archivo se compone de **registros**, cada uno con:
1. **User-agent** ‚Üí especifica el bot (ej. `Googlebot`, `Bingbot`, `*` para todos).  
2. **Directivas** ‚Üí instrucciones para ese bot.

### Directivas comunes
| Directiva   | Descripci√≥n | Ejemplo |
|-------------|-------------|---------|
| `Disallow`  | Bloquea acceso a rutas | `Disallow: /admin/` |
| `Allow`     | Permite acceso, incluso si hay reglas generales de `Disallow` | `Allow: /public/` |
| `Crawl-delay` | Tiempo (segundos) entre peticiones de un bot | `Crawl-delay: 10` |
| `Sitemap`   | URL del sitemap en XML | `Sitemap: https://www.example.com/sitemap.xml` |

---

## Ejemplo de robots.txt
```txt
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/

User-agent: Googlebot
Crawl-delay: 10

Sitemap: https://www.example.com/sitemap.xml
```

### Interpretaci√≥n
- Todos los bots no pueden acceder a `/admin/` y `/private/`.  
- Todos los bots s√≠ pueden acceder a `/public/`.  
- `Googlebot` debe esperar 10 segundos entre peticiones.  
- Hay un `sitemap.xml` publicado para indexaci√≥n.  

---

## Importancia de robots.txt
- No es obligatorio ‚Üí bots maliciosos pueden ignorarlo.  
- **Razones para respetarlo**:
  - Evitar sobrecargar el servidor.  
  - Proteger datos sensibles de la indexaci√≥n.  
  - Cumplir normas legales o de t√©rminos de servicio.  

---

## robots.txt en Web Recon
- üïµÔ∏è **Descubrimiento de directorios ocultos**: rutas marcadas como privadas pueden revelar paneles de admin, backups, etc.  
- üó∫Ô∏è **Mapeo de la estructura**: inferir √°reas del sitio no enlazadas en la navegaci√≥n p√∫blica.  
- ü™§ **Detecci√≥n de honeypots**: algunos sitios incluyen rutas trampa en robots.txt para detectar bots maliciosos.  
